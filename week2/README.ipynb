{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7264487e",
   "metadata": {},
   "source": [
    "\n",
    "# Week 2 — Baseline Models & Time-Series Evaluation\n",
    "\n",
    "## Learning goals\n",
    "This week is about **building baseline models** and learning **evaluation discipline**.\n",
    "\n",
    "By the end of Week 2, you should be able to:\n",
    "- Build meaningful **baseline predictors** for returns\n",
    "- Evaluate models using **walk-forward (time-series) validation**\n",
    "- Understand why **prediction accuracy ≠ trading performance**\n",
    "- Avoid common sources of **look-ahead bias**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336809c1",
   "metadata": {},
   "source": [
    "\n",
    "## What you will build\n",
    "\n",
    "You will implement and compare:\n",
    "\n",
    "### Naive baselines\n",
    "1. Zero-return predictor  (No model)\n",
    "2. Rolling historical mean predictor\n",
    "\n",
    "### Linear model\n",
    "1. Ordinary Least Squares (OLS)  \n",
    "\n",
    "### Tree models(optionally)\n",
    "1. Decision Trees\n",
    "2. Random Forests\n",
    "\n",
    "You will evaluate them using **strictly forward-looking splits**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fe5deb",
   "metadata": {},
   "source": [
    "## Why start with baselines?\n",
    "\n",
    "Financial return prediction is an extremely noisy problem.  \n",
    "Before using complex models, it is essential to understand how **simple strategies behave**.\n",
    "\n",
    "Baselines help us:\n",
    "- Set a realistic performance reference\n",
    "- Detect data leakage and evaluation bugs\n",
    "- Understand whether a model is learning signal or just noise\n",
    "\n",
    "In finance, it is common for simple baselines to be **_surprisingly hard to beat_**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4abc451d",
   "metadata": {},
   "source": [
    "## Naive baselines\n",
    "\n",
    "### 1. Zero-return predictor\n",
    "The zero predictor always predicts a return of zero.\n",
    "\n",
    "This corresponds to the assumption that the price is constant\n",
    "\n",
    "Why this matters:\n",
    "- Daily asset returns have mean close to zero\n",
    "- This predictor often achieves reasonable RMSE\n",
    "- It provides a strong sanity check for all models\n",
    "\n",
    "If a model cannot beat this baseline **out-of-sample**, it is likely useless.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab831905",
   "metadata": {},
   "source": [
    "### 2. Rolling historical mean predictor\n",
    "This predictor estimates the expected return as the average of recent past returns:\n",
    "\n",
    "$$\n",
    "\\hat r_{t+1} = \\frac{1}{W}\\sum_{i=t-W+1}^t r_i\n",
    "$$\n",
    "\n",
    "Interpretation:\n",
    "- Assumes short-term return persistence\n",
    "- Equivalent to a very simple momentum model\n",
    "- Sensitive to window length\n",
    "\n",
    "This baseline introduces **time dependence** without using any machine learning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4524564d",
   "metadata": {},
   "source": [
    "### 3. OLS Predictor\n",
    "\n",
    "For each day \\( t \\), you construct a feature vector using only information\n",
    "available up to that day:\n",
    "\n",
    "$$\n",
    "x_t =\n",
    "\\begin{bmatrix}\n",
    "r_t \\\\\n",
    "r_{t-1} \\\\\n",
    "\\text{rolling mean}_t \\\\\n",
    "\\text{rolling volatility}_t \\\\\n",
    "\\vdots\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Important:**\n",
    "- All features use data from time $\\le t $\n",
    "- No future returns appear in $x_t$\n",
    "\n",
    "This is why the model is said to be **forward-looking**.\n",
    "\n",
    "---\n",
    "\n",
    "### Training data construction\n",
    "\n",
    "From historical data, you build a dataset of input–output pairs:\n",
    "\n",
    "$$\n",
    "\\{(x_1, y_1), (x_2, y_2), \\dots, (x_T, y_T)\\}\n",
    "$$\n",
    "\n",
    "where the target is defined as the **next-day return**:\n",
    "\n",
    "$$\n",
    "y_t = r_{t+1}\n",
    "$$\n",
    "\n",
    "Each example answers the question:\n",
    "> “Using information available at day \\( t \\), can we predict the return at day \\( t+1 \\)?”\n",
    "\n",
    "---\n",
    "\n",
    "### OLS objective\n",
    "\n",
    "Ordinary Least Squares (OLS) finds weights \\( w \\) by minimizing mean squared error:\n",
    "\n",
    "$$\n",
    "\\min_w \\sum_{t=1}^{T} \\left( y_t - w^\\top x_t \\right)^2\n",
    "$$\n",
    "\n",
    "**Interpretation:**\n",
    "- Learn a linear mapping from today’s features to tomorrow’s return\n",
    "- This is standard regression, but applied to time-ordered financial data\n",
    "\n",
    "---\n",
    "\n",
    "### Making a prediction\n",
    "\n",
    "At the **end of day \\( T \\)**:\n",
    "- You observe the feature vector \\( x_T \\)\n",
    "- You compute the prediction:\n",
    "\n",
    "$$\n",
    "\\hat r_{T+1} = w^\\top x_T\n",
    "$$\n",
    "\n",
    "This value is the model’s **predicted return for the next trading day**.\n",
    "\n",
    "In general figuring out what features to use in $x$ can be very hard, will require experimentation and a lot of intuition. For this stage we do not need you to figure out which feature gives us best returns.\n",
    "\n",
    "So the feature vector $ x_t $ should use:\n",
    "\n",
    "- $r_t$\n",
    "- $r_{t-1}$\n",
    "- 20-day rolling mean of returns\n",
    "- 20-day rolling volatility of returns\n",
    "- 5-day momentum (cumulative return)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97d8d9e",
   "metadata": {},
   "source": [
    "### 4. Random Forest Predictor (optional)\n",
    "\n",
    "Random Forests are **tree-based ensemble models** that combine predictions from\n",
    "many decision trees to reduce variance and improve stability.\n",
    "\n",
    "As with OLS, the goal is to predict the **next-day return** using information\n",
    "available up to the current day.\n",
    "\n",
    "---\n",
    "\n",
    "### Feature construction\n",
    "\n",
    "For each day $t$, you construct a feature vector using only past and present\n",
    "information:\n",
    "\n",
    "$$\n",
    "x_t =\n",
    "\\begin{bmatrix}\n",
    "r_t \\\\\n",
    "r_{t-1} \\\\\n",
    "\\text{rolling mean}_t \\\\\n",
    "\\text{rolling volatility}_t \\\\\n",
    "\\text{momentum}_t \\\\\n",
    "\\vdots\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "**Important:**\n",
    "- All features use data from time $\\le t$\n",
    "- No future returns appear in $x_t$\n",
    "\n",
    "The same feature set used for linear models is reused here to ensure a fair\n",
    "comparison across models.\n",
    "\n",
    "---\n",
    "\n",
    "### Training data construction\n",
    "\n",
    "From historical data, you build a dataset of input–output pairs:\n",
    "\n",
    "$$\n",
    "\\{(x_1, y_1), (x_2, y_2), \\dots, (x_T, y_T)\\}\n",
    "$$\n",
    "\n",
    "where the target is defined as the **next-day return**:\n",
    "\n",
    "$$\n",
    "y_t = r_{t+1}\n",
    "$$\n",
    "\n",
    "Each example answers the question:\n",
    "> “Using information available at day $t$, can we predict the return at day $t+1$?”\n",
    "\n",
    "---\n",
    "\n",
    "### Random Forest objective (intuition)\n",
    "\n",
    "A Random Forest consists of many decision trees, each trained on a bootstrap\n",
    "sample of the data and a random subset of features.\n",
    "\n",
    "Each tree produces a prediction $\\hat r_{t+1}^{(k)}$, and the final forecast is\n",
    "the average across trees:\n",
    "\n",
    "$$\n",
    "\\hat r_{t+1}\n",
    "=\n",
    "\\frac{1}{K}\n",
    "\\sum_{k=1}^{K} \\hat r_{t+1}^{(k)}\n",
    "$$\n",
    "\n",
    "where $K$ is the number of trees in the forest.\n",
    "\n",
    "---\n",
    "\n",
    "### Interpretation\n",
    "\n",
    "- Random Forests can capture **nonlinear relationships** and feature interactions\n",
    "- They perform implicit feature selection\n",
    "- They are far more flexible than linear models\n",
    "\n",
    "However, in financial time series:\n",
    "- Signal-to-noise ratios are extremely low\n",
    "- Relationships are unstable over time\n",
    "- High-capacity models often **overfit noise**\n",
    "\n",
    "For this reason, Random Forests are included as a **learning tool**, not as a\n",
    "recommended production model.\n",
    "\n",
    "---\n",
    "\n",
    "### Making a prediction\n",
    "\n",
    "At the **end of day $T$**:\n",
    "- You observe the feature vector $x_T$\n",
    "- Each tree produces a prediction\n",
    "- The Random Forest averages these predictions:\n",
    "\n",
    "$$\n",
    "\\hat r_{T+1} = \\text{RF}(x_T)\n",
    "$$\n",
    "\n",
    "This value is the model’s **predicted return for the next trading day**.\n",
    "\n",
    "---\n",
    "\n",
    "### Feature choice for this assignment\n",
    "\n",
    "In general, designing good features for tree-based models is difficult and\n",
    "requires extensive experimentation.\n",
    "\n",
    "For this stage, you should **not** search for optimal features.\n",
    "\n",
    "Use the following fixed feature set:\n",
    "\n",
    "- $r_t$\n",
    "- $r_{t-1}$\n",
    "- 20-day rolling mean of returns\n",
    "- 20-day rolling volatility of returns\n",
    "- 5-day momentum (cumulative return)\n",
    "\n",
    "This allows us to focus on **evaluation discipline** rather than feature engineering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92191cda",
   "metadata": {},
   "source": [
    "## Evaluating Prediction Accuracy\n",
    "\n",
    "The goal of prediction evaluation is to measure how close the model’s predicted\n",
    "returns are to the realized future returns.\n",
    "\n",
    "Let:\n",
    "- $y_t = r_{t+1}$ be the true next-day return\n",
    "- $\\hat y_t$ be the predicted return made using information available at time $t$\n",
    "\n",
    "Evaluation must always be performed **out-of-sample** using forward-looking data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c4d7cef",
   "metadata": {},
   "source": [
    "### Mean Squared Error (MSE)\n",
    "\n",
    "The Mean Squared Error measures the average squared difference between predicted\n",
    "and realized returns:\n",
    "\n",
    "$$\n",
    "\\text{MSE}\n",
    "=\n",
    "\\frac{1}{N}\n",
    "\\sum_{t=1}^{N}\n",
    "(\\hat y_t - y_t)^2\n",
    "$$\n",
    "\n",
    "Properties:\n",
    "- Penalizes large errors heavily\n",
    "- Sensitive to outliers\n",
    "- Commonly used for regression models\n",
    "\n",
    "Lower MSE indicates more accurate predictions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a97927",
   "metadata": {},
   "source": [
    "### Root Mean Squared Error (RMSE)\n",
    "\n",
    "RMSE is the square root of MSE:\n",
    "\n",
    "$$\n",
    "\\text{RMSE} = \\sqrt{\\text{MSE}}\n",
    "$$\n",
    "\n",
    "Why RMSE is preferred:\n",
    "- Same units as returns\n",
    "- Easier to interpret than MSE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848ef160",
   "metadata": {},
   "source": [
    "Create plots to compare the prediction accuracy of models "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9a40dd",
   "metadata": {},
   "source": [
    "## References & Further Reading\n",
    "\n",
    "### Python & Data Handling\n",
    "- **pandas documentation**  \n",
    "  https://pandas.pydata.org/docs/  \n",
    "  (Used for time-series handling, rolling windows, and feature construction)\n",
    "\n",
    "---\n",
    "\n",
    "### Linear Models (OLS & Ridge)\n",
    "- **scikit-learn — Linear Regression (OLS)**  \n",
    "  https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares\n",
    "\n",
    "- **scikit-learn — Ridge Regression**  \n",
    "  https://scikit-learn.org/stable/modules/linear_model.html#ridge-regression\n",
    "\n",
    "---\n",
    "\n",
    "### Tree-Based Models (Optional)\n",
    "- **scikit-learn — Decision Trees**  \n",
    "  https://scikit-learn.org/stable/modules/tree.html\n",
    "\n",
    "- **scikit-learn — Random Forest Regressor**  \n",
    "  https://scikit-learn.org/stable/modules/ensemble.html#random-forests\n",
    "\n",
    "---\n",
    "\n",
    "### Statistics & Intuition (Highly Recommended)\n",
    "- **StatQuest with Josh Starmer (YouTube)**  \n",
    "  https://www.youtube.com/@statquest  \n",
    "  (Clear intuition for regression, bias–variance tradeoff, and tree-based models)\n",
    "\n",
    "Recommended StatQuest videos:\n",
    "- Linear Regression  \n",
    "  https://www.youtube.com/watch?v=nk2CQITm_eo\n",
    "- Ridge Regression  \n",
    "  https://www.youtube.com/watch?v=Q81RR3yKn30\n",
    "- Decision Trees  \n",
    "  https://www.youtube.com/watch?v=7VeUPuFGJHk\n",
    "- Random Forests  \n",
    "  https://www.youtube.com/watch?v=J4Wdy0Wc_xQ\n",
    "\n",
    "---\n",
    "\n",
    "### MIT OpenCourseWare (Optional, Deeper Theory)\n",
    "- **MIT OCW — 18.065 Matrix Methods in Data Analysis**  \n",
    "  https://ocw.mit.edu/courses/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/\n",
    "\n",
    "- **MIT OCW — 6.036 Introduction to Machine Learning**  \n",
    "  https://ocw.mit.edu/courses/6-036-introduction-to-machine-learning-fall-2020/\n",
    "\n",
    "These resources provide deeper mathematical intuition but are **not required**\n",
    "to complete this assignment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e9f9d8c",
   "metadata": {},
   "source": [
    "\n",
    "## Files\n",
    "- `task2.ipynb` — complete all coding and questions here\n",
    "\n",
    "Reuse the data and features created in **Week 1**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc1af06",
   "metadata": {},
   "source": [
    "\n",
    "## Note\n",
    "\n",
    "- ❌ Random train/test splits are **not allowed**\n",
    "- ✅ Only time-series / walk-forward evaluation is permitted\n",
    "- ✅ Training data must come strictly **before** test data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5339f8",
   "metadata": {},
   "source": [
    "\n",
    "## Submission instructions\n",
    "\n",
    "1. Complete `task2.ipynb`\n",
    "2. Create a branch:\n",
    "```bash\n",
    "git checkout -b submission-week2\n",
    "git push -u origin submission-week2\n",
    "```\n",
    "1. Submit the link to the *most recent commit* to this branch (before submission date).\n",
    "\n",
    "That commit will be treated as a **fixed snapshot**.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
